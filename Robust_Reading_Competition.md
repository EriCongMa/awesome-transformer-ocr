# Robust Reading Competition Readme

[TOC]

## DocVQA 2020-2021

[Homepage](https://rrc.cvc.uab.es/?ch=17)

Document Visual Question Answering: The DocVQA challenge is a continuous effort linked to various events. The challenge was originally organised in the context of the [CVPR 2020 Workshop on Text and Documents in the Deep Learning Era](https://cvpr2020text.wordpress.com/). From this first event a paper with the results was presented in the Document Analysis Systems International Workshop that can be found [here](https://arxiv.org/abs/2008.08899). The second edition will take place in the context of the [Int. Conference on Document Analysis and Recognition (ICDAR) 2021](https://icdar2021.org/).

- Task 1 - Single Document VQA: Is a typical VQA style task, where natural language questions are defined over single documents, and an answer needs to be generated by interpreting the document image. No list of pre-defined responses will be given, hence the problem cannot be easily treated as an n-way classification task.
  - **UPDATE**: This task remains open for the community to keep working on, but it won't be part of the new ICDAR2021 competition.
  - Publications: [DocVQA: A Dataset for VQA on Document Images](https://openaccess.thecvf.com/content/WACV2021/papers/Mathew_DocVQA_A_Dataset_for_VQA_on_Document_Images_WACV_2021_paper.pdf)

- Task 2 - Document Collection VQA: Is a retrieval-style task where given a question, the aim is to identify and retrieve all the documents in a large document collection that are relevant to answering this question as well as the answer.
  - **UPDATE**: This task has been redesigned for the ICDAR2021 edition to take into account both the evidences provided and the question answering performance of participating methods.
  - Publications: [Document Collection Visual Question Answering](https://arxiv.org/pdf/2104.14336.pdf)

- **Task 3 - Infographics VQA (NEW!)**: This is a new task introduced as part of the 2021 challenge (ICDAR 2021 edition). The task is similar to the task1 where questions are posed on Infographic images instead of business and industry documents. More details of the task can be found under "Tasks" tab.



## ST-VQA 2019

[Homepage](https://rrc.cvc.uab.es/?ch=11)

ICDAR 2019 Robust Reading Challenge on Scene Text Visual Question Answering:

- Task 1 - Strongly Contextualised
  In this first task, the participants will be provided with a different list of possible answers for each image. The list will comprise some of the words that appear within the image, plus some extra dictionary words . As such, each image will contain a relatively small but different set of possible answers. For the example image above, the participant would be given a list including the words below, plus some dictionary words: [ Public, Market, Center, Coca-Cola, Farmers, Enjoy, … ] 

- Task 2 - Weakly Contextualised
  In this task, the participants will be provided the full list of possible answers for the complete dataset and complemented with some dictionary words. Although the list of possible answers will be the same (a static list) for all the images within the dataset, the list is considerably larger than the set of answers from the previous task. The dictionary is comprised by 30,000 words formed by collecting all the 22k ground truth words plus 8k generated vocabulary.

- Task 3 - Open Dictionary
  The end-to-end task is the most generic and challenging one, since no set of answers is provided a priori. The submitted methods for this task should be able to generate the correct answers by analysing the image's visual context and reading and understanding all image contained textual information.



## MLT 2019

[Homepage]()

ICDAR 2019 Robust Reading Challenge on Multi-lingual scene text detection and recognition:

- Task-1: Multi-script text detection
  In this task, a participant method should be able to generalize to detecting text of different scripts. The input to this task is scene images with embedded text in various languages, and the required detection is at word level.

- Task-2: Cropped Word Script identification
  The text in our dataset images appears in 10 different languages, some of them share the same script. Additionally, punctuation and some math symbols sometimes appear as separate words, those words are assigned a special script class called "Symbols". Hence, we have a total of 8 different scripts. We have excluded the words that have "Mixed" script for this task. We have also excluded all "don't care" words whether they have an identified script or not.

- Task-3: Joint text detection and script identification
  This task combines all the preparation steps needed for multi-script text recognition. A participant method should take as input a full scene image, and then find the bounding boxes of all the words, and the information about each word in terms of script id.

- Task-4: End-to-End text detection and recognition
  This is a very challenging task of a unified OCR for multiple-languages. The end-to-end scene text detection and recognition task in multi-language setting is coherent with its English counterparts. Given an input scene image, the objective is to localize a set of bounding boxes and their corresponding transcriptions.



## LSVT 2019

[Homepage](https://rrc.cvc.uab.es/?ch=16)

ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling:

- Task1：Text detection
  This task is to evaluate the text detection performance, where candidates' methods are expected to localize text from street view images at the level of text lines.

  - Input: Full street view images

  - Output: Locations of text lines in quadrangles or polygons for all the text instances.

- Task2：End-to-end text spotting
  The main objective of this task is to detect and recognize every text instance in the provided image in an end-to-end manner.

  - Input: Full street view images

  - Output: Locations of text lines in quadrangles or polygons and the corresponding recognized results for all the text instances in the image.



## ✨ ArT 2019

[Homepage](https://rrc.cvc.uab.es/?ch=14)

 ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text:

- Task 1: Scene Text Detection
  The main objective of this task is to detect the location of every text instance given an input image, which is similar to all the previous RRC scene text detection tasks. The input of this task is strictly constrained to image only, no other form of input is allowed to aid the model in the process of detecting the text instances.
  - Input: Scene text image
  - Output: Spatial location of every text instance at word-level for Latin scripts, and line-level for Chinese scripts.

- Task 2: Scene Text Recognition
  The main objective of this task is to recognize every character in a cropped image patch, which is also one of the common tasks in previous RRC. Considering the fact that the research in Chinese scripts text recognition is relatively immature compared to Latin scripts, we decided to further break down T2 into two subcategories:

  - T2.1 - Latin script only,
  - T2.2 - Latin and Chinese scripts.
  - We hope that such a split could make this task friendlier for non-Chinese, as the main problem that we are trying to address in this competition is the challenge of arbitrary-shaped text.

  - Input: Cropped image patch with text instance.
  - Output: A string of predicted characters.

- Task 3: Scene Text Spotting
  The main objective of this task is to detect and recognize every text instance in the provided image in an end-to-end manner. Similar to RRC 2017, a generic vocabulary list (90K common English words) will be provided as a reference for this challenge. Identical to T2, we break T3 down into two sub categories:
  - T3.1 Latin script only text spotting,
  - T3.2 Latin and Chinese scripts text spotting. 
  - Input: Scene text image
  - Output: Spatial location of every text instance at word-level for Latin scripts, and line-level for Chinese scripts together with the predicted word for each detection.



## SROIE 2019

[Homepage](https://rrc.cvc.uab.es/?ch=13)

ICDAR 2019 Robust Reading Challenge on Scanned Receipts OCR and Information Extraction:

- Task 1 - Scanned Receipt Text Localisation
  Task Description
  Localizing and recognizing text are conventional tasks that has appeared in many previous competitions, such as the ICDAR Robust Reading Competition (RRC) 2013, ICDAR RRC 2015 and ICDAR RRC 2017 [1][2]. The aim of this task is to accurately localize texts with 4 vertices. The text localization ground truth will be at least at the level of words. Participants will be asked to submit a zip file containing results for all test images.



- Task 2 - Scanned Receipt OCR
  Task Description
  The aim of this task is to accurately recognize the text in a receipt image. No localisation information is provided, or is required. Instead the participants are required to offer a list of words recognised in the image. The task will be restricted to words comprising Latin characters and numbers only.
  - The ground truth needed to train for this task is the list of words that appear in the transcriptions. In order to obtain the ground truth for this task, you should tokenise all strings splitting on space. For example the string “Date: 12/3/56” should be tokenised “Date:”, “12/3/56”. While the string “Date: 12 / 3 / 56” should be tokenised “Date:” “12”, “/”, “3”, “/”, “56”.



- Task 3 - Key Information Extraction from Scanned Receipts
  Task Description
  The aim of this task is to extract texts of a number of key fields from given receipts, and save the texts for each receipt image in a json file with format shown in Figure 3. Participants will be asked to submit a zip file containing results for all test invoice images.



## ✨ ReCTS 2019

[Homepage](https://rrc.cvc.uab.es/?ch=12)

 ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard

- Task 1. Character Recognition in the Signboard
  The aim of this task is to recognize characters from the cropped character image.

- Task 2. Text Line Recognition in the Signboard
  The cropped text line images and the coordinates of the polygon bounding boxes in the images are also given. 

  - Submission Format
    Participants will be asked to submit a txt file containing results for all test images. The results format is:

    - img_name,transcription

    - eg. test_000001.jpg,炸鸡

  - Evaluation Metrics

    We use the Normalized Edit Distance as the evaluation metric for text line recognition, which is formulated as follows:
    $$
    N = 1-\frac{1}{N}\sum_{i=1}^{N}D(s_i, \hat{s_i})/\text{max}(s_i, \hat{s_i})
    $$
    ![test22.png](https://rrc.cvc.uab.es/files/test22.png)

- Task 3. Text Line Detection in the Signboard
  The aim of this task is to localize text lines in the signboard. The input image is the full signboard images.

- Task 4. End-to-End Text Spotting in the Signboard
  The aim of this task is to localize and recognize every text instance in the signboard. The input image is the full signboard images.





























